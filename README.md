![WhatsApp Image 2024-10-05 at 10 04 33_41ed6854](https://github.com/user-attachments/assets/c41ba752-1b28-4c5c-a08a-9a59b4410dbd)

# EDITH (EVEN DEAF, I'M THE HERO)

## Aim
To develop an efficient model of a Sign Language interpreter exclusive for Indian Sign Language(ISL) 

## Abstract

Sign language is a visual language that utilizes hand shapes, movements, facial expressions, and body postures to communicate meaning. Unlike spoken languages, it communicates through gestures and non-manual signals like eye gaze and body posture. Different regions have unique sign languages, such as **ASL**, **BSL**, and **ISL**, each with distinct grammar. Advances in technology, like gesture recognition and AI, aim to improve communication between signers and non-signers, fostering inclusivity.

## Project Description

Indian Sign Language (ISL) is the primary form of communication for many Deaf and hard-of-hearing individuals in India. It is a visual-gestural language that uses hand signs, facial expressions, and body movements to convey meaning, much like other sign languages worldwide. However, ISL is unique compared to Western counterparts like American Sign Language (ASL) and British Sign Language (BSL) because it incorporates signs from Indian languages such as Hindi, Tamil, and others. ISL has often been under-recognized and under-documented compared to Western sign languages.

ISL has its own grammar, syntax, and sentence structure like other sign languages. The word order in ISL typically differs from that of spoken Indian languages, and it relies heavily on non-manual signals like facial expressions to convey tone, questions, or emphasis. Many ISL signs are deeply connected to Indian cultural contexts. Given Indiaâ€™s diversity, there are regional variations, but mutual intelligibility is generally maintained across the country.

There remains a shortage of trained ISL interpreters in India, which creates challenges for deaf individuals in healthcare, legal settings, and general communication.

To address this gap, our model, EDITH, aims to function as a bridge, filling the need for professionally trained ISL interpreters.

Communication is the exchange of thoughts and messages through various means such as speech, signals, behavior, and visuals. Deaf and Mute (D&M) individuals use their hands to express gestures. EDITH serves as an interpreter between D&M individuals and others, helping the latter understand the signs made by the former.

## Pre-Requisites
1. Python
2. React.js
3. OpenCV
4. Basic knowledge of Machine Learning

## Methology

### Data collection
    Here, the dataset is defined by the programmer. This specifically uses OpenCV to collect the datasets for the signs of words.
### Data processing
    Here, The collected dataset is processed thoroughly by the Machine and it learns the joints in our hands and also the gestures that we are posing.
### Training
        Now, The processed dataset is gone thorugh a set of codes where the Machine deeply learns the indepth meaning of the image where the code is given by the programmer.
### Testing
        After every process, the Machine is now able to figure out the gesture which the user is posing and gives out the word and its speech that the sign corresponds to and can do vice versa: ie Our EDITH is capable of changing the given input word into its corresponding sign as a video.
## Webpage Development
        We have completed the model by developing a website with React.js and Tailwind CSS which allows the user to use our model in an effective and productive manner.

## Webpage

![WhatsApp Image 2024-10-05 at 10 00 14_306545a5](https://github.com/user-attachments/assets/526a99f8-e2d3-4ba4-ad3d-376fafcb9bce)
![WhatsApp Image 2024-10-05 at 10 01 12_e0c83d28](https://github.com/user-attachments/assets/f1931c5e-7c7d-4655-8867-cad6611441c4)
![WhatsApp Image 2024-10-05 at 10 01 31_521a2d58](https://github.com/user-attachments/assets/67011264-61d3-4783-97ec-3be28c4647bb)

## Features

This prototype is special for indian sign language as we believe this is the first dedicated model which can interpret in both ways, ie from *Sign* to *Word* and *Speech* and *Word* to *A Sign video*. This builds the bridge between Deaf & Mute People and Ordinary People.
This model also shines bright as a learning and teaching tool for Indian Sign Language.

## Future Impact
- Social Impact:
    Makes communication between sign language users and non-signers easier and more accessible.
- Accessibility:
    Real-time sign language recognition can improve access to services in environments such as hospitals, police stations, government offices, and public transportation, where communication is crucial but interpreters may not always be available

## Conclusion
Our EDITH is designed to bridge the communication gap between sign language users and non-signers by leveraging AI to translate hand gestures into text in real time. This project aims to foster inclusivity and empower the deaf and hard-of-hearing community by providing a tool that enables seamless communication without the need for interpreters.

